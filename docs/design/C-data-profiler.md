# Design Doc C: Data Profiler (Zero-Context Data Analysis)

## 1. Overview

The Data Profiler powers Mode B: the user drops in a dataset with NO
explanation, and the system autonomously determines what the data is, what
study produced it, what analyses are appropriate, and executes them.

**Design principle: Opinionated, not inquisitive.** The system makes
defensible analytical choices and documents its reasoning. It never asks
the user for clarification.

**Modules covered:**

| Module | File |
|--------|------|
| Data Profiler | `core/src/dataProfiler.ts` |
| Study Reconstructor | `core/src/studyReconstructor.ts` |
| Analysis Planner | `core/src/analysisPlanner.ts` |
| Profiler Types | `core/src/dataProfiler/types.ts` |
| Profiler Prompts | `core/src/prompts/hci/dataProfilePrompt.ts` |
|                   | `core/src/prompts/hci/studyReconstructPrompt.ts` |
|                   | `core/src/prompts/hci/analysisPlanPrompt.ts` |

---

## 2. Mode B Pipeline Flow

When the InputAnalyzer (Doc A) detects a data file, the pipeline enters
Mode B. The SCOPE and DESIGN phases are replaced by a combined profiling
step, and COLLECT is skipped entirely:

```
Mode A (normal):   SCOPE → DESIGN → COLLECT → ANALYZE → SYNTHESIZE → REVIEW
Mode B (data-in):  PROFILE → (skip) → (skip) → ANALYZE → SYNTHESIZE → REVIEW
                   ^^^^^^^^
                   Replaces SCOPE + DESIGN + COLLECT
```

The PROFILE phase itself has three sequential substeps:

```
PROFILE:
  1. Data Profiling     → What is this data? (structure, types, patterns)
  2. Study Reconstruction → What study produced this? (reverse-engineer the design)
  3. Analysis Planning  → What analyses should we run? (statistical plan)
```

After PROFILE completes, the pipeline continues with ANALYZE as normal.

---

## 3. DAG Integration

Mode B generates this DAG:

```typescript
// Built by PipelineOrchestrator when mode === REAL_DATA
const modeBNodes: PhaseNode[] = [
  {
    id: 'scope/data-profile',
    type: PhaseType.SCOPE,
    title: 'Profile input dataset',
    dependsOn: [],
    ...
  },
  {
    id: 'scope/study-reconstruct',
    type: PhaseType.SCOPE,
    title: 'Reconstruct study design from data',
    dependsOn: ['scope/data-profile'],
    ...
  },
  {
    id: 'design/analysis-plan',
    type: PhaseType.DESIGN,
    title: 'Generate analysis plan',
    dependsOn: ['scope/study-reconstruct'],
    ...
  },
  // COLLECT is skipped -- mark as completed immediately
  // ANALYZE, SYNTHESIZE, REVIEW proceed as normal
];
```

---

## 4. Data Models

### 4.1 Data Profile

```typescript
/**
 * Structural profile of a dataset.
 * Generated by the Data Profiler step without any human context.
 */
export interface DataProfile {
  /** Source file info */
  source: {
    fileName: string;
    fileFormat: 'csv' | 'tsv' | 'json' | 'jsonl' | 'xlsx' | 'parquet';
    fileSize: number;
    encoding: string;
  };

  /** Row/column overview */
  shape: {
    rowCount: number;
    columnCount: number;
  };

  /** Per-column analysis */
  columns: ColumnProfile[];

  /** Detected patterns */
  patterns: {
    /** Does this look like it has a participant ID column? */
    hasParticipantId: boolean;
    participantIdColumn?: string;
    uniqueParticipantCount?: number;

    /** Does this look like repeated measures? */
    isRepeatedMeasures: boolean;
    repeatedMeasuresIndicator?: string;

    /** Are there condition/group columns? */
    hasConditionColumn: boolean;
    conditionColumns?: string[];
    conditionLevels?: Record<string, string[]>;

    /** Are there timestamp columns? */
    hasTimestamps: boolean;
    timestampColumns?: string[];

    /** Does this look like survey data? */
    isSurveyData: boolean;
    surveyScaleDetected?: string;   // e.g., "Likert 5-point", "Likert 7-point"

    /** Known questionnaire detection */
    knownQuestionnaires?: KnownQuestionnaireMatch[];
  };

  /** Data quality */
  quality: {
    missingValueCount: number;
    missingValuePercentage: number;
    columnsWithMissing: string[];
    duplicateRowCount: number;
    outlierColumns: string[];     // Columns with detected outliers
  };
}

export interface ColumnProfile {
  name: string;
  inferredType: 'numeric' | 'categorical' | 'ordinal' | 'text' | 'timestamp' | 'id' | 'boolean';
  originalType: string;         // Raw dtype from the file

  /** For numeric columns */
  numericStats?: {
    mean: number;
    median: number;
    std: number;
    min: number;
    max: number;
    skewness: number;
    kurtosis: number;
    normalityPValue: number;    // Shapiro-Wilk p-value
    isNormal: boolean;          // p > 0.05
  };

  /** For categorical/ordinal columns */
  categoricalStats?: {
    uniqueValues: string[];
    valueCounts: Record<string, number>;
    mode: string;
    isBalanced: boolean;        // roughly equal distribution
  };

  /** Missingness */
  missingCount: number;
  missingPercentage: number;

  /** Role inference */
  inferredRole: 'independent_variable' | 'dependent_variable' | 'covariate'
    | 'participant_id' | 'condition' | 'timestamp' | 'unknown';
}
```

### 4.2 Known Questionnaire Detection

```typescript
/**
 * Recognized standard HCI questionnaires.
 * Matched by column name patterns.
 */
export interface KnownQuestionnaireMatch {
  name: string;           // e.g., "System Usability Scale (SUS)"
  confidence: number;     // 0-1
  matchedColumns: string[];
  expectedColumns: number;
  scoringMethod: string;  // How to compute the score
}

/**
 * Built-in questionnaire signatures for auto-detection.
 */
export const KNOWN_QUESTIONNAIRES: QuestionnaireSignature[] = [
  {
    name: 'System Usability Scale (SUS)',
    columnPatterns: [/sus[_\s]?\d{1,2}/i, /usability[_\s]?\d{1,2}/i],
    expectedItemCount: 10,
    scaleRange: [1, 5],
    scoringMethod: 'sus_standard',  // (sum of odd items - 5) + (25 - sum of even items) * 2.5
    reference: 'Brooke, J. (1996)',
  },
  {
    name: 'NASA Task Load Index (NASA-TLX)',
    columnPatterns: [/tlx/i, /mental[_\s]?demand/i, /physical[_\s]?demand/i,
                     /temporal[_\s]?demand/i, /performance/i, /effort/i, /frustration/i],
    expectedItemCount: 6,
    scaleRange: [0, 100],    // or [1, 21] for paper version
    scoringMethod: 'nasa_tlx_raw',  // raw TLX = mean of 6 subscales
    reference: 'Hart & Staveland (1988)',
  },
  {
    name: 'User Experience Questionnaire (UEQ)',
    columnPatterns: [/ueq/i, /attractiveness/i, /perspicuity/i,
                     /efficiency/i, /dependability/i, /stimulation/i, /novelty/i],
    expectedItemCount: 26,
    scaleRange: [-3, 3],
    scoringMethod: 'ueq_standard',
    reference: 'Laugwitz et al. (2008)',
  },
  {
    name: 'AttrakDiff',
    columnPatterns: [/attrakdiff/i, /pragmatic/i, /hedonic/i],
    expectedItemCount: 28,
    scaleRange: [-3, 3],
    scoringMethod: 'attrakdiff_standard',
    reference: 'Hassenzahl et al. (2003)',
  },
  // Additional: PSSUQ, QUIS, UMUX, UMUX-LITE, CSI, etc.
];
```

### 4.3 Reconstructed Study

```typescript
/**
 * Reverse-engineered study design from data structure alone.
 */
export interface ReconstructedStudy {
  /** Confidence in the reconstruction (0-1) */
  confidence: number;

  /** Inferred research questions */
  researchQuestions: string[];

  /** Study design type */
  designType: 'between_subjects' | 'within_subjects' | 'mixed' | 'single_group'
    | 'correlational' | 'longitudinal' | 'unknown';

  /** Identified variables */
  variables: {
    independent: VariableSpec[];
    dependent: VariableSpec[];
    covariates: VariableSpec[];
    participantId: string;
  };

  /** Factor structure (for factorial designs) */
  factors?: {
    name: string;
    levels: string[];
    type: 'between' | 'within';
  }[];

  /** Sample description */
  sample: {
    size: number;
    perCondition?: Record<string, number>;
  };

  /** Reasoning: why the system inferred this design */
  reasoning: string;
}

export interface VariableSpec {
  columnName: string;
  label: string;           // Human-readable name
  type: 'numeric' | 'categorical' | 'ordinal';
  role: string;            // More specific role description
}
```

### 4.4 Analysis Plan

```typescript
/**
 * Statistical analysis plan generated from the data profile
 * and reconstructed study design.
 */
export interface AnalysisPlan {
  /** Plan sections, executed in order */
  steps: AnalysisStep[];

  /** Statistical framework used */
  framework: 'frequentist' | 'bayesian' | 'both';

  /** Significance level */
  alpha: number;  // default: 0.05

  /** Multiple comparison correction method */
  correctionMethod?: 'bonferroni' | 'holm' | 'fdr' | 'none';

  /** Reasoning for each decision */
  decisions: AnalysisDecision[];
}

export interface AnalysisStep {
  id: string;
  title: string;
  category: 'descriptive' | 'assumption_check' | 'inferential'
    | 'post_hoc' | 'effect_size' | 'visualization';

  /** What test/procedure to run */
  method: string;  // e.g., "two_way_anova", "mann_whitney_u", "shapiro_wilk"

  /** Variables involved */
  variables: {
    dependent: string;
    independent?: string[];
    grouping?: string;
  };

  /** Language to implement in */
  implementation: 'python' | 'r';

  /** Specific code template ID or inline code */
  code?: string;

  /** What this step produces */
  outputType: 'table' | 'figure' | 'statistic' | 'text';
}

export interface AnalysisDecision {
  question: string;     // e.g., "Parametric or non-parametric?"
  decision: string;     // e.g., "Non-parametric (Mann-Whitney U)"
  reasoning: string;    // e.g., "Shapiro-Wilk p < 0.05 for completion_time, distribution is right-skewed"
}
```

---

## 5. Module: Data Profiler (`dataProfiler.ts`)

### 5.1 Responsibility

Structural analysis of the raw data. Pure computation, no AI needed.
Runs locally (not in Docker) as a preprocessing step.

### 5.2 Interface

```typescript
export class DataProfiler {
  /**
   * Profile a data file without any contextual information.
   *
   * Steps:
   * 1. Detect file format and read data
   * 2. Infer column types
   * 3. Compute per-column statistics
   * 4. Detect structural patterns (IDs, conditions, repeated measures)
   * 5. Match against known questionnaire signatures
   * 6. Assess data quality
   *
   * @param filePath Path to the data file
   * @returns DataProfile
   */
  static async profile(filePath: string): Promise<DataProfile>;

  /**
   * Profile a directory of data files.
   * Determines which files are data vs. metadata vs. codebooks.
   */
  static async profileDirectory(dirPath: string): Promise<{
    mainDataFile: string;
    supplementaryFiles: string[];
    profile: DataProfile;
  }>;
}
```

### 5.3 Column Type Inference Algorithm

```
For each column:
  1. Sample first 100 non-null values
  2. Type detection cascade:
     a. All values are timestamps? → timestamp
     b. All unique, monotonically increasing? → id
     c. All values parseable as numbers?
        - Integer-only, range 1-5 or 1-7 or 1-10? → ordinal (Likert)
        - Continuous distribution? → numeric
     d. Unique value count < 10 AND count < 20% of rows? → categorical
     e. Unique value count < 20 AND values have natural ordering? → ordinal
     f. Otherwise → text
  3. Role inference:
     a. Column named *_id, id, participant, subject, pid → participant_id
     b. Column named condition, group, treatment, intervention → condition
     c. Column named time, date, timestamp, trial → timestamp
     d. Ordinal/numeric columns after condition columns → dependent_variable
     e. Low-cardinality categorical → independent_variable
     f. Everything else → unknown (let Study Reconstructor decide)
```

### 5.4 Known Questionnaire Matching

```
For each known questionnaire signature:
  1. Check if column names match patterns
  2. Count matched columns
  3. If matched_count / expected_count > 0.7:
     - Confidence = matched_count / expected_count
     - Record match
  4. Also check for columns named with questionnaire abbreviation
     (e.g., "SUS_1", "SUS_2", ..., "SUS_10")
```

### 5.5 Implementation Detail: No Docker Needed

Data profiling is deterministic computation. It runs locally using Bun's
built-in file reading capabilities. The heavy lifting is:

- CSV/TSV parsing: Use a lightweight parser (e.g., `csv-parse` or Bun native)
- JSON: Native `JSON.parse`
- XLSX: Use `xlsx` npm package
- Statistics: Implement basic stats (mean, median, std, Shapiro-Wilk) in TS
  or spawn a quick Python/R script

For Shapiro-Wilk and normality tests, the simplest approach is to generate
a small Python script and run it via `bun shell` or `child_process`:

```typescript
// Generate and run a Python script for statistical tests
async function runStatisticalTests(data: number[]): Promise<{
  shapiroPValue: number;
  skewness: number;
  kurtosis: number;
}> {
  const script = `
import json, sys
import numpy as np
from scipy import stats

data = json.loads(sys.argv[1])
stat, p = stats.shapiro(data[:5000])  # Shapiro-Wilk max 5000
skew = float(stats.skew(data))
kurt = float(stats.kurtosis(data))
print(json.dumps({"shapiroPValue": p, "skewness": skew, "kurtosis": kurt}))
`;
  // Execute via subprocess
}
```

---

## 6. Module: Study Reconstructor (`studyReconstructor.ts`)

### 6.1 Responsibility

Given a DataProfile, use AI to reverse-engineer the study design.
This is where the LLM's knowledge of HCI research methods is critical.

### 6.2 Interface

```typescript
export class StudyReconstructor {
  /**
   * Reconstruct the study design from data profile alone.
   * Runs in a Docker container with an AI agent.
   */
  static async reconstruct(
    profile: DataProfile,
    config: HCIConfig,
  ): Promise<ReconstructedStudy>;
}
```

### 6.3 Prompt Design (`studyReconstructPrompt.ts`)

```typescript
export function studyReconstructPrompt(profile: DataProfile): string {
  return `
You are an expert HCI research methodologist. You are given the structural
profile of a dataset. You have NO other information about this study.
Your task is to reverse-engineer the study design.

## Data Profile
${JSON.stringify(profile, null, 2)}

## Your Task

1. **Identify the study design type**
   Look at:
   - Number of condition columns and their levels → factorial design?
   - Does participant_id repeat? → within-subjects
   - Does each participant_id appear once? → between-subjects
   - Are there timestamp columns with repeated entries? → longitudinal
   - No condition column at all? → correlational or single-group

2. **Identify variables and their roles**
   - Independent variables: condition columns, group assignments
   - Dependent variables: measured outcomes (numeric columns, questionnaire scores)
   - Covariates: demographic variables, pre-test scores
   - Clearly separate IVs from DVs based on column naming and cardinality

3. **Infer research questions**
   Based on the IV-DV mapping, formulate 1-3 specific research questions.
   Example: "Does [condition] affect [dependent variable]?"
   Be specific: use actual column names.

4. **Identify known questionnaires**
   Known matches found: ${JSON.stringify(profile.patterns.knownQuestionnaires || [])}
   For each match, confirm or reject based on your expertise.
   Add scoring instructions if confirmed.

5. **Describe your reasoning**
   For each inference, explain WHY you concluded this. The reasoning
   will be included in the paper's methodology section.

## Output Format
Write a JSON object conforming to ReconstructedStudy interface.
`.trim();
}
```

### 6.4 Inference Strategies

| Data Pattern | Inference |
|-------------|-----------|
| 1 condition column, 2 levels, participant ID unique | Between-subjects, 2-group comparison |
| 1 condition column, 3+ levels, participant ID unique | Between-subjects, multi-group (one-way ANOVA) |
| 2 condition columns, participant ID unique | Between-subjects factorial |
| Participant ID repeats N times, N matches condition levels | Within-subjects |
| Participant ID repeats, + between-group column | Mixed design |
| No condition column, all numeric | Correlational |
| Timestamp column + repeated participant ID | Longitudinal |
| SUS columns detected | Usability study |
| NASA-TLX columns detected | Workload assessment study |

---

## 7. Module: Analysis Planner (`analysisPlanner.ts`)

### 7.1 Responsibility

Given a DataProfile and ReconstructedStudy, generate a concrete statistical
analysis plan. The system makes ALL analytical decisions autonomously.

### 7.2 Interface

```typescript
export class AnalysisPlanner {
  /**
   * Generate a complete analysis plan.
   * Runs in Docker with AI agent, but uses deterministic decision
   * rules where possible (test selection flowchart).
   */
  static async plan(
    profile: DataProfile,
    study: ReconstructedStudy,
    config: HCIConfig,
  ): Promise<AnalysisPlan>;
}
```

### 7.3 Decision Flowchart (Deterministic Part)

The statistical test selection follows standard decision rules:

```
For each DV:
  1. Check normality (Shapiro-Wilk from DataProfile)
     │
     ├─ Normal (p > 0.05)
     │    │
     │    ├─ 1 IV, 2 groups
     │    │    ├─ between-subjects → independent t-test
     │    │    └─ within-subjects  → paired t-test
     │    │
     │    ├─ 1 IV, 3+ groups
     │    │    ├─ between-subjects → one-way ANOVA + Tukey HSD
     │    │    └─ within-subjects  → repeated measures ANOVA + pairwise t
     │    │
     │    ├─ 2+ IVs
     │    │    ├─ all between      → factorial ANOVA
     │    │    ├─ all within       → repeated measures ANOVA
     │    │    └─ mixed            → mixed ANOVA
     │    │
     │    └─ no IV (correlational) → Pearson correlation
     │
     └─ Not normal (p <= 0.05)
          │
          ├─ 1 IV, 2 groups
          │    ├─ between-subjects → Mann-Whitney U
          │    └─ within-subjects  → Wilcoxon signed-rank
          │
          ├─ 1 IV, 3+ groups
          │    ├─ between-subjects → Kruskal-Wallis + Dunn's test
          │    └─ within-subjects  → Friedman test + Nemenyi
          │
          └─ no IV (correlational) → Spearman correlation

For all inferential tests:
  - Report effect size:
    - t-test → Cohen's d
    - ANOVA → eta-squared (η²) or partial eta-squared
    - Mann-Whitney → rank-biserial correlation
    - Chi-square → Cramer's V
  - Apply multiple comparison correction if > 3 comparisons
```

### 7.4 AI-Assisted Part

The AI agent handles non-deterministic decisions:

- Whether to include covariates (ANCOVA vs. ANOVA)
- Whether to run additional exploratory analyses
- How to handle outliers (remove, winsorize, report both)
- Whether to report Bayesian equivalents
- How to interpret interaction effects
- Which visualizations are most informative for this data

### 7.5 Analysis Plan Output Example

```json
{
  "steps": [
    {
      "id": "desc-1",
      "title": "Descriptive statistics by condition",
      "category": "descriptive",
      "method": "descriptive_by_group",
      "variables": { "dependent": "completion_time", "grouping": "interface_type" },
      "implementation": "python",
      "outputType": "table"
    },
    {
      "id": "norm-1",
      "title": "Normality test for completion_time",
      "category": "assumption_check",
      "method": "shapiro_wilk",
      "variables": { "dependent": "completion_time" },
      "implementation": "python",
      "outputType": "statistic"
    },
    {
      "id": "inf-1",
      "title": "Compare completion_time across interface types",
      "category": "inferential",
      "method": "one_way_anova",
      "variables": { "dependent": "completion_time", "independent": ["interface_type"] },
      "implementation": "python",
      "outputType": "statistic"
    },
    {
      "id": "post-1",
      "title": "Post-hoc pairwise comparisons",
      "category": "post_hoc",
      "method": "tukey_hsd",
      "variables": { "dependent": "completion_time", "grouping": "interface_type" },
      "implementation": "python",
      "outputType": "table"
    },
    {
      "id": "eff-1",
      "title": "Effect size for completion_time",
      "category": "effect_size",
      "method": "eta_squared",
      "variables": { "dependent": "completion_time", "independent": ["interface_type"] },
      "implementation": "python",
      "outputType": "statistic"
    },
    {
      "id": "vis-1",
      "title": "Box plot of completion_time by condition",
      "category": "visualization",
      "method": "boxplot",
      "variables": { "dependent": "completion_time", "grouping": "interface_type" },
      "implementation": "python",
      "outputType": "figure"
    },
    {
      "id": "sus-1",
      "title": "Compute SUS scores",
      "category": "descriptive",
      "method": "sus_standard",
      "variables": { "dependent": "sus_score" },
      "implementation": "python",
      "outputType": "table"
    }
  ],
  "framework": "frequentist",
  "alpha": 0.05,
  "correctionMethod": "holm",
  "decisions": [
    {
      "question": "Parametric or non-parametric for completion_time?",
      "decision": "Parametric (one-way ANOVA)",
      "reasoning": "Shapiro-Wilk p = 0.23 > 0.05, data is approximately normal. Levene's test p = 0.41, homogeneity of variance holds."
    },
    {
      "question": "Multiple comparison correction method?",
      "decision": "Holm-Bonferroni",
      "reasoning": "3 pairwise comparisons. Holm is less conservative than Bonferroni while still controlling FWER."
    }
  ]
}
```

---

## 8. Execution: ANALYZE Phase

After the Analysis Planner produces the plan, the ANALYZE phase executes it.
Each analysis step becomes a subtask in the DAG:

```typescript
// Generated by maybeExpandDAG when design/analysis-plan completes
const analyzeNodes: PhaseNode[] = analysisPlan.steps.map(step => ({
  id: `analyze/${step.id}`,
  type: PhaseType.ANALYZE,
  title: step.title,
  description: `Execute analysis step: ${step.method}`,
  // Descriptive stats have no dependencies.
  // Assumption checks depend on descriptive.
  // Inferential depends on assumption checks.
  // Post-hoc depends on inferential.
  // Visualizations can run in parallel with inferential.
  dependsOn: computeAnalysisDependencies(step, analysisPlan),
  outputArtifacts: [`analyze/${step.id}/result`],
  inputArtifacts: ['collect/raw_data', 'design/analysis-plan'],
  status: PhaseStatus.PENDING,
}));

// Final aggregation: compile all results
analyzeNodes.push({
  id: 'analyze/compile',
  type: PhaseType.ANALYZE,
  title: 'Compile analysis results',
  dependsOn: analyzeNodes.map(n => n.id),
  outputArtifacts: ['analyze/results', 'analyze/results_apa'],
  ...
});
```

Each analysis step runs in a Docker container with Python/R:

```
Container setup:
  1. Install python3, scipy, statsmodels, matplotlib, seaborn, pandas
  2. Copy raw_data into /app/data/
  3. Copy generated analysis script into /app/scripts/
  4. Run the script
  5. Collect output (tables, figures, JSON results)
```

---

## 9. Code Generation

The Analysis Planner generates executable Python scripts for each step.

### 9.1 Script Template Structure

```python
#!/usr/bin/env python3
"""
Auto-generated analysis script: {step.title}
Method: {step.method}
Generated by FSC-HCI Data Profiler
"""

import pandas as pd
import numpy as np
from scipy import stats
import json
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('/app/data/raw_data.csv')

# ===== ANALYSIS =====
{generated_analysis_code}

# ===== OUTPUT =====
results = {generated_results_collection}

with open('/app/output/results.json', 'w') as f:
    json.dump(results, f, indent=2, default=str)

# Save figure if applicable
plt.savefig('/app/output/figure.png', dpi=300, bbox_inches='tight')
```

### 9.2 Example: ANOVA Code Generation

```python
# For method: "one_way_anova"
# DV: completion_time, IV: interface_type

groups = [group['completion_time'].values
          for name, group in df.groupby('interface_type')]

# Assumption: Levene's test
levene_stat, levene_p = stats.levene(*groups)

# Main test
f_stat, p_value = stats.f_oneway(*groups)

# Effect size: eta-squared
ss_between = sum(len(g) * (np.mean(g) - df['completion_time'].mean())**2
                 for g in groups)
ss_total = sum((df['completion_time'] - df['completion_time'].mean())**2)
eta_squared = ss_between / ss_total

results = {
    "test": "one_way_anova",
    "f_statistic": float(f_stat),
    "p_value": float(p_value),
    "eta_squared": float(eta_squared),
    "levene_p": float(levene_p),
    "significant": p_value < 0.05,
    "df_between": len(groups) - 1,
    "df_within": len(df) - len(groups),
}
```

---

## 10. APA Formatting

The final step of ANALYZE generates APA-formatted result descriptions:

```typescript
/**
 * Convert statistical results into APA 7th edition formatted text.
 */
export function formatAPA(result: AnalysisResult): string {
  // Example output:
  // "A one-way ANOVA revealed a statistically significant difference in
  //  completion time across interface types, F(2, 87) = 4.52, p = .013,
  //  η² = .094. Post-hoc comparisons using Tukey's HSD indicated that
  //  the gesture interface (M = 12.3, SD = 3.1) was significantly faster
  //  than the voice interface (M = 15.7, SD = 4.2), p = .009, d = 0.92."
}
```

This is generated by the AI agent in the `analyze/compile` node, reading all
individual analysis results and producing:
- `results.json` (structured data)
- `results_apa.md` (formatted prose for the paper)
- All figures in `figures/`

---

## 11. Supported Data Formats

| Format | Extension | Parser | Notes |
|--------|-----------|--------|-------|
| CSV | `.csv` | Built-in or `csv-parse` | Auto-detect delimiter |
| TSV | `.tsv` | Same as CSV | Tab delimiter |
| JSON | `.json` | Native `JSON.parse` | Array of objects |
| JSON Lines | `.jsonl` | Line-by-line parse | One object per line |
| Excel | `.xlsx` | `xlsx` package | First sheet by default |
| Parquet | `.parquet` | `parquet-wasm` or Python | Columnar format |

### 11.1 Multi-file Support

If the user provides a directory:

```
data/
├── responses.csv         ← Main data file (largest, most columns)
├── demographics.csv      ← Supplementary (has participant_id, merge on it)
├── codebook.txt          ← Metadata (parsed for column descriptions)
└── README.txt            ← Study description (parsed if available)
```

The profiler identifies the main data file by row/column count and detects
join keys for supplementary files.

---

## 12. Edge Cases and Decisions

| Situation | System Decision |
|-----------|----------------|
| Cannot determine study design | Report as "exploratory analysis", run descriptive stats + correlation matrix |
| Data has too many conditions (>10 levels) | Treat as regression, not ANOVA |
| Very small sample (N < 10) | Use non-parametric tests regardless of normality, note limitation |
| Missing data > 20% in a column | Exclude column from analysis, report in limitations |
| Missing data 5-20% | Multiple imputation or listwise deletion, document choice |
| Outliers detected (>3 SD) | Report both with and without outliers |
| Unbalanced groups | Use Type III SS for ANOVA, note imbalance |
| Mixed data types in column | Coerce to most common type, warn in report |
| Single-item vs. multi-item scale | Auto-compute composite scores for detected questionnaires |

All decisions are logged in `AnalysisPlan.decisions[]` and included in the
paper's methodology section.

---

## 13. Integration with Pipeline

```
Mode B timeline:

User drops in "experiment_data.csv"
  │
  ├─ InputAnalyzer.detect() → InputType.DATASET, Mode.REAL_DATA
  │
  ├─ scope/data-profile     → DataProfile (local, no Docker)
  │
  ├─ scope/study-reconstruct → ReconstructedStudy (Docker + AI)
  │
  ├─ design/analysis-plan   → AnalysisPlan (Docker + AI)
  │
  ├─ [COLLECT skipped]
  │
  ├─ analyze/*              → Results, figures, APA text (Docker + Python)
  │
  ├─ synthesize/*           → Paper sections (Docker + AI)
  │
  └─ review/*               → Final review (Docker + AI)

Total user interaction: drag-and-drop one file. Nothing else.
```
